{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS (EDA)\n",
    "#### EDA is an important step in data analysis process. It not only involves examining the data to understand its structure, detect patterns, spot anomalies, test hypotheses, and check assumptions but it also helps in summarizing the main characteristics of the data, often using visual methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's an example of how EDA works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/ambigaur/Desktop/Ambi_coding/python/Prasunet_DS/Titanic_train.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Understanding dataset and data distribution : \n",
    "##### Checking the few rows, summary statistics, data types and distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=20, figsize=(20,15))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Graphical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Categorical Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "sns.countplot(data=df, x='Survived')\n",
    "plt.title('Survival Count')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "sns.countplot(data=df, x='Pclass')\n",
    "plt.title('Passenger class count')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "sns.countplot(data=df, x='Sex')\n",
    "plt.title('Gender Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Bivariate Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='Pclass', y='Survived')\n",
    "plt.title('Survival rate by Passenger class')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data=df, x='Sex', y='Survived')\n",
    "plt.title('Survival rate by different gender')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data=df, x='Embarked', y='Survived')\n",
    "plt.title('Survival rate by port of embarkation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.boxplot(data=df, x='Survived', y='Age')\n",
    "plt.title('Age Distribution by Survival')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(data=df, x='Survived', y=\"Fare\")\n",
    "plt.title('Fare Distribution by Survival')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.scatterplot(data=df, x='Age', y='Fare', hue='Survived')\n",
    "plt.title('Age Vs Fare Coloured by Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(numeric_df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing value analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "#### 1.Basic Information: Start by understanding the structure and summary statistics of the data to get an overall sense of what it contains.\n",
    "#### 2.Data Distribution: Examine how numerical features are distributed to identify patterns and outliers.\n",
    "#### 3.Feature Analysis: Analyze categorical features using count plots, study relationships between survival and other features, and use box plots and scatter plots to explore numerical features. This helps you see the distribution, frequency, and interactions of different variables.\n",
    "#### 4.Correlation Analysis: Identify correlations between features to understand how variables are related and possibly dependent on each other.\n",
    "#### 5.Missing Values Analysis: Visualize and understand the pattern of missing data to determine how to handle these gaps in your analysis or modeling.\n",
    "#### Now we move on to our next step Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "#### Data Cleaning is the process of detecting and then correcting, either by removing or using other techniques, errors and inconsistencies from the data to improve the quality of the data. Data Cleaning is often reffered as data scrapping or data cleansing. There are several ways through which we can identify and rectify different irregularities present in the data.\n",
    "\n",
    "## 1. Removal of Duplicate data:\n",
    "#### Indentification and removal of duplicate entries in a dataset.\n",
    "## 2. Handling missing values:\n",
    "#### Indentification of missing values and then processing them. There are few ways by which we can handle missing values either by removing them or replacing them with values like mean, median, mode of the given data columns.\n",
    "## 3. Dealing with Outliers: \n",
    "#### Identifying and handling outliers, which are data points significantly different from others. Depending on the context, outliers may be corrected, removed, or kept.\n",
    "## 4. Standardizing Data: \n",
    "#### Making sure that data is consistent and uniformaly formatted like standardizing date formats, text capitalization, and units of measurement.\n",
    "## 5. Removal of Irrelevant data:\n",
    "#### Looking after data that doesn't align with the analysis and desicion making processes and then removing them.\n",
    "## 6. Validating Data:\n",
    "#### Ensuring that the data conforms to defined rules and constraints, such as data type constraints, range constraints, and unique constraints.\n",
    "## 7. Fixing Inaccurate Data: \n",
    "#### Fixing any errors or inaccuracies in the data, such as misspelled words, incorrect entries, or outdated information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "df['Sex'].fillna(df['Sex'].mode()[0], inplace=True)\n",
    "df['Survived'].fillna(df['Survived'].mode()[0], inplace=True)\n",
    "df['Cabin'].fillna(\"Unknown\", inplace=True)\n",
    "df['Pclass'].fillna(df['Pclass'].mode()[0], inplace=True)\n",
    "df['Parch'].fillna(df['Parch'].mode()[0], inplace=True)\n",
    "df['SibSp'].fillna(df['SibSp'].mode()[0], inplace=True)///"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({'Age': df['Age'].median()}, inplace=True)\n",
    "df.fillna({'Embarked': df['Embarked'].mode()[0]}, inplace=True)\n",
    "df.fillna({'Fare': df['Fare'].median()}, inplace=True)\n",
    "df.fillna({'Sex': df['Sex'].mode()[0]}, inplace=True)\n",
    "df.fillna({'Survived': df['Survived'].mode()[0]}, inplace=True)\n",
    "df.fillna({'Cabin': \"Unknown\"}, inplace=True)\n",
    "df.fillna({'Pclass': df['Pclass'].mode()[0]}, inplace=True)\n",
    "df.fillna({'Parch': df['Parch'].mode()[0]}, inplace=True)\n",
    "df.fillna({'SibSp': df['SibSp'].mode()[0]}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dealing with Outliers\n",
    "##### Here we are calculating the upper_limit by using the formula: UP= mean + 3*Standard_deviation\n",
    "##### After calculating the upper limit we are simply replacing the numbers greater than that with upper limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_upper_limit= df['Age'].mean() + 3* df['Age'].std()\n",
    "df.loc[df['Age']>age_upper_limit, 'Age'] = age_upper_limit\n",
    "\n",
    "fare_upper_limit= df['Fare'].mean() + 3* df['Fare'].std()\n",
    "df.loc[df['Fare']> fare_upper_limit, 'Fare']= fare_upper_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Standardizing data: Here we are converting the values in sex column to lower space for proper functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex']= df['Sex'].apply(lambda x: x.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Removal of irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Validating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['Pclass'].isin([1, 2, 3]).all(), \"Pclass contains invalid values\"\n",
    "assert df['Survived'].isin([0, 1]).all(), \"Survived contains invalid values\"\n",
    "assert df['Sex'].isin(['male', 'female']).all(), \"Sex contains invalid values\"\n",
    "assert df['Embarked'].isin(['C', 'Q', 'S']).all(), \"Embarked contains invalid values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Fixing Inaccurate data: There's no such data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Titanic_Cleaned_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
